ðŸ“„ Technical Report: Development of CNN Architectures for Waste Classification
1. Objective and Personal Contribution

The project aims to develop and compare different Deep Learning architectures for the automatic classification of waste into 9 distinct categories. My main contribution was not limited to the application of existing models but focused on the design and implementation from scratch of complex neural architectures to analyze their behavior compared to the state of the art.
2. Data Analysis and Preprocessing Pipeline

I began with an Exploratory Data Analysis (EDA) which revealed a native image resolution of 524Ã—524 pixels. To balance computational load with the need to preserve structural details (e.g., cardboard texture vs. plastic), I implemented a preprocessing pipeline that:

    Performs downsampling to 224Ã—224, standardizing the input for all architectures.

    Applies statistical normalization based on ImageNet parameters.

    Introduces Data Augmentation techniques (rotations, flips) in the training set to mitigate overfitting, given the limited number of samples per class (~25k total).

    Manages the stratified division of the dataset into Train (70%), Validation (15%), and Test (15%), ensuring a statistically robust evaluation.

3. Architecture Development (Project Core)

To conduct a rigorous comparative study, I wrote the code for three distinct architectures, implementing modular logic (Factory Pattern) that allows swapping models with a single parameter:

    A. Sequential Baseline (VGG-Style): I built a classic CNN composed of 4 sequential convolutional blocks. This model serves as a baseline to quantify the performance of a network without residual connections.

    B. Custom ResNet18 (Manual Implementation): This represents the core of my research work. Instead of importing ResNet from external libraries, I implemented the architecture line-by-line:

        I coded the Residual Block (BasicBlock), manually handling the skip connections and tensor dimension adaptation.

        I assembled the 4 stages of ResNet18 following the standard [2, 2, 2, 2] topology.

        Original Contribution: I modified the standard architecture by inserting a Dropout layer (p=0.5) before the final classifier. This design choice was dictated by the need to further regularize the model, adapting it to a smaller dataset compared to the one ResNet was originally conceived for (ImageNet).

    C. Transfer Learning (The Benchmark): I used a ResNet18 pre-trained on ImageNet as a comparison term ("Champion"), performing fine-tuning only on the final Fully Connected layer.

4. Training and Evaluation Methodology

I developed a robust training engine (train_model) that includes:

    Automatic Checkpointing: The system saves model weights only when validation accuracy improves, preventing the saving of overfitting models in the final epochs.

    History Logging: Saving of Loss and Accuracy metrics for each epoch, allowing the generation of comparative plots.

The final evaluation is performed on the Test Set (unseen data) using advanced metrics:

    Classification Report: Analysis of Precision, Recall, and F1-Score for every single waste class.

    Confusion Matrix: To visually diagnose frequent errors (e.g., confusion between Glass and Plastic) and validate the effectiveness of the different architectures.